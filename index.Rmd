---
title: "Classifying Common Mistakes in Weight Lifting Motion Using Fitness Tracker Data"
author: "Wayne Witzke"
output: 
  html_document:
    keep_md: true
    code_folding: hide
---

```{r global_options}
# This analysis presumes that there is a specific directory structure in place.
# That structure is defined here.
knitr::opts_chunk$set( fig.path = "figure/" );
raw.data.dir = "raw";
raw.training.file = file.path( raw.data.dir, "RawTrainingData" );
raw.test.file = file.path( raw.data.dir, "RawTestData" );
suppressMessages(
{
    library(data.table);
    library(caret);
    library(randomForest);
    library(doParallel);
});
set.seed(1869304218);
cluster = makeCluster( detectCores() - 1 );
registerDoParallel( cluster );
```

## Synopsis
We explore the possibility of using personal fitness tracker data to classify
errors in exercise performance. Existing data on weight lifting exercises was
used to create several machine learning models, which were then tested against
out-of-sample data to determine accuracy. A ensemble model approach was found
to yield extremely good results.

## Data Processing

### Data collection and processing overview
This analysis uses data from the Weight Lifting Exercise dataset created by
Velloso, E et al. Training data from this dataset is programmatically
downloaded from [this](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)
location, and test data (also from this dataset) is programmatically downloaded
from [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv).
More information about this data and the group that collected it can be found
[here](http://groupware.les.inf.puc-rio.br/har).

```{r GetRawFile_function, results = "hold"}
if ( !dir.exists( raw.data.dir ) )
{
    dir.create( raw.data.dir, recursive = TRUE );
}

# This function will fetch a file from an online location, timestamp it, and
# then return the timestamp. It only does this if the `output.file` doesn't
# already exist. If it does, then it will merely return the timestamp for the
# already saved raw data set.
GetRawFile = function( url, output.file )
{
    timestamp = "";
    timestamp.file = paste( output.file, "timestamp", sep = "." );

    if ( !file.exists( output.file ) )
    {
        cat( paste( "Downloading", output.file, "\n" ) );
        timestamp = date();
        download.file(
            url = url,
            destfile = output.file
        );
        writeLines( timestamp, timestamp.file );
    }
    else
    {
        cat( paste( "Using existing", output.file, "\n" ) );
        timestamp = readLines( timestamp.file );
    }

    suppressWarnings(
        unzip( output.file, overwrite = FALSE, exdir = raw.data.dir )
    );

    return(timestamp);
}

cat(
    "Raw training data downloaded on",
    GetRawFile("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
    raw.training.file),
    "\n"
);
cat(
    "Raw test data downloaded on",
    GetRawFile("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
    raw.test.file),
    "\n"
);
```

The data is comprised of a number of accelerometer and gyroscope measurements,
time sequence information, some categorical values, and summary rows that
divide the observations up into different collection windows. It describes the
movements associated with a specific exercise repeatedly performed by test
subjects in several different ways. Our outcome variable, `classe`, classifies
how the exercises were pereformed, either correctly (`classe = A`), or
incorrectly (`classe != A`), where the exercise was performed with one of four
common mistakes. The full structure of the data can be seen in figure 1.

#####Figure 1: Dataset structure
```{r read_decompressed_weather_table}
# Dropping the first column of row labels.
na.st = c("","NA");
exercise.table =
    fread(
        raw.training.file,
        na.strings = na.st,
        drop = c(1),
        showProgress = FALSE
    );
exercise.test =
    fread(
        raw.test.file,
        na.strings = na.st,
        drop = c(1),
        showProgress = FALSE
    );
str(exercise.table, list.len = 1000000000);
```

### Creating Data Slices
Before we begin to tidy the data in this analysis, we segment the data into
training and validation sets. Based on the structure of the data and the nature
of the final test, and because we wish to perform multiple tests, the data will
have a simple 80/20 slice, with 80% of the data used for training, and 20% used
for validation. The selection of validation data is done using random sampling.
The final test data was selected independent of this analysis.

```{r create_train_validation}
inTrain = createDataPartition( exercise.table$classe, p = 0.8, list = FALSE );
exercise.train = exercise.table[inTrain, ];
exercise.validate = exercise.table[-inTrain, ];
```

In addition to creating our own validation set, we will be doing
10-fold-cross-validation as part of every model (hence the need for only 20%
for the final validation step).

```{r train_control_creation}
#   This appears to do 10-fold cross validation by default, and allow parallel
#   processing to boot. We do, however, appear to need to calculate a bunch of
#   seeds for reproducability.
seeds = vector( mode = "list", length = 11 ); # Apparently the length is
                                              # number*repeat, which are 10 and
                                              # 1 by default in trainControl.
for( ii in 1:11 )
{
    seeds[[ii]] = sample.int(.Machine$integer.max, ncol(exercise.train)-1);
}
#   And apparently we want one value in each seed vector for each tuning
#   parameters, which is just the number of variables in our data right now.
fitControl = trainControl(method = "cv", seed = seeds);
```

### Tidying the data
A cursory examination of this data shows a number of potential problems. For 
instance, many of the variables have a large number of `NA` values. We remove
any variables that have more than 80% of their values missing. 

```{r find_NA_columns}
keep.cols =
    which(
        unlist(
            exercise.train[ , lapply(.SD,function(x) sum(is.na(x)) < 0.2*.N)]
        )
    );
exercise.train = exercise.train[ , keep.cols, with=FALSE];
```

This leaves no missing values in the training set.
```{r evaluate_remaining_NAs}
cat(
    "There are",
    sum(sapply(exercise.train, function(x) sum(is.na(x)))),
    "NAs remaining.\n"
);
```

In addition, several variables were loaded as character data. One should be a
date/time variable, while the others should be factor variables.

```{r correct_column_types}
exercise.train$cvtd_timestamp =
    as.POSIXct(exercise.train$cvtd_timestamp, format="%d/%m/%Y %H:%M");
exercise.train$user_name = factor(exercise.train$user_name);
exercise.train$new_window = factor(exercise.train$new_window);
exercise.train$classe = factor(exercise.train$classe);
```

## Model Creation
Because our outcome variable is categorical, we use random forests, boosting,
and linear discriminant analysis to find the best model for prediction. We also
combine these predictors to create an aggregate model to compare against the
individual models.

```{r train_RF_model, cache=TRUE}
model.RF =
    suppressMessages(
        train(
            classe ~ .,
            data = exercise.train,
            method = "rf",
            trControl = fitControl 
        )
    );
pred.train.RF = predict(model.RF, exercise.train);
```
```{r train_GBM_model, cache=TRUE}
model.GBM = 
    suppressMessages(
        train(
            classe ~ .,
            data = exercise.train,
            method = "gbm",
            trControl = fitControl,
            verbose = FALSE
        )
    );
pred.train.GBM = predict(model.GBM, exercise.train);
```
```{r train_LDA_model, cache=TRUE}
model.LDA = 
    suppressMessages(
        suppressWarnings(
            train(
                classe ~ .,
                data = exercise.train,
                method = "lda",
                trControl = fitControl 
            )
        )
    );
pred.train.LDA = predict(model.LDA, exercise.train);
```
```{r train_ensemble_model, cache=TRUE}
combined.predict =
    data.frame(
        p1 = pred.train.RF,
        p2 = pred.train.GBM,
        p3 = pred.train.LDA,
        classe = exercise.train$classe
    );

model.combined = 
    suppressMessages(
        train(
            classe ~ .,
            data = combined.predict,
            method = "gbm",
            trControl = fitControl,
            verbose = FALSE
        )
    );
pred.train.combined = predict(model.combined, combined.predict);

```
```{r cleanup_cores}
#   We'll clean up here to avoid warning messages.
stopCluster(cluster);
```

Figure 2 shows the in-sample accuracy of the various methods. Clearly, with
100% in-sample accuracy, both random forest and the combined methods seem to
produce the strongest results.

#####Figure 2: Model fit in-sample accuracy
```{r base_model_evaluation, results = "hold"}
cat("Random Forest model:\n");
confusionMatrix(pred.train.RF, exercise.train$classe);
cat("\nBoosted model:\n");
confusionMatrix(pred.train.GBM, exercise.train$classe);
cat("\nLinear Discriminant Analysis model:\n");
confusionMatrix(pred.train.LDA, exercise.train$classe);
cat("\nCombined model:\n");
confusionMatrix(pred.train.combined, exercise.train$classe);
```

## Model Validation and Selection
In order to select the model that is most likely to fit external data, we
now validate the models using the validation dataset captured earlier. Before
we can do this, we have to prepare it in the same way the training data was
prepared.

```{r set_up_validation_data}
exercise.validate = exercise.validate[ , keep.cols, with=FALSE];

exercise.validate$cvtd_timestamp =
    as.POSIXct(exercise.validate$cvtd_timestamp, format="%d/%m/%Y %H:%M");
exercise.validate$user_name = factor(exercise.validate$user_name);
exercise.validate$new_window = factor(exercise.validate$new_window);
exercise.validate$classe = factor(exercise.validate$classe);
```

As can be seen in figure 3, once again both the random forest and combined
models perform identically well, and better than the other two models. Since,
theoretically, it should be a better predictor, the combined model will be used
on the final test data.

#####Figure 3: Estimated out of sample accuracy
```{r apply_model_to_validation, cache=TRUE, results = "hold"}
pred.validate.RF = predict(model.RF, exercise.validate);
pred.validate.GBM = predict(model.GBM, exercise.validate);
pred.validate.LDA = predict(model.LDA, exercise.validate);
combined.validate.predict =
    data.frame(
        p1 = pred.validate.RF,
        p2 = pred.validate.GBM,
        p3 = pred.validate.LDA,
        classe = exercise.validate$classe
    );
pred.validate.combined = predict(model.combined, combined.validate.predict);
cat("Random Forest model:\n");
confusionMatrix(pred.validate.RF, exercise.validate$classe);
cat("\nBoosted model:\n");
confusionMatrix(pred.validate.GBM, exercise.validate$classe);
cat("\nLinear Discriminant Analysis model:\n");
confusionMatrix(pred.validate.LDA, exercise.validate$classe);
cat("\nCombined model:\n");
confusionMatrix(pred.validate.combined, exercise.validate$classe);
```
## Conclusions: Final Test Data Prediction and Model Evaluation
Applying this model to the test set, we get the following predictions.

```{r apply_model_to_test, cache=TRUE, results = "hold"}
exercise.test = exercise.test[ , keep.cols, with=FALSE];

exercise.test$cvtd_timestamp =
    as.POSIXct(exercise.test$cvtd_timestamp, format="%d/%m/%Y %H:%M");
exercise.test$user_name = factor(exercise.test$user_name);
exercise.test$new_window = factor(exercise.test$new_window);

pred.test.RF = predict(model.RF, exercise.test);
pred.test.GBM = predict(model.GBM, exercise.test);
pred.test.LDA = predict(model.LDA, exercise.test);
combined.test.predict =
    data.frame(
        p1 = pred.test.RF,
        p2 = pred.test.GBM,
        p3 = pred.test.LDA
    );
final.prediction = predict(model.combined, combined.test.predict);
names(final.prediction) = 1:20;
print(final.prediction);
```

This is apparently 100% correct.

It is no surprise that we were able to get such highly accurate results. The
test data apparently comes directly from the original dataset, which, as a time
series, has highly correlated data windows in it. Likely if we were to apply
these models to data generated outside of this initial study, we would not see
such high performance in the model fit.

## Appendix

#### System Information
This analysis was performed using the hardware and software listed in this
section.

```{r system_info}
sessionInfo();
```



